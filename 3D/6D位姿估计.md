# 6D位姿估计

## 定义

### 相机6D位姿



- 相机6D位姿是指拍摄当前图像时刻，相机坐标系相对于世界坐标系发生的平移和旋转变换。世界坐标系可以定义在任意位置。相机6D位姿通常用世界系到相机系的RT变换来表示：

$$
T_{c}=R_{c w} * T_{w}+t_{c w}
$$

- $R_{cw}$代表由世界系到相机系的旋转（或者说相机系相对于世界系的旋转），$t_{cw}$代表由世界系到相机系的平移，$T_w$表示世界系下的3D点，$T_c$表示相机系下的3D点。

  

### 物体6D位姿

- 物体6D位姿是指拍摄当前图像时刻，相机坐标系相对于原始物体所在的世界系，发生的平移和旋转变换。原始物体可以放在世界系的任何位置，而且通常将物体本身的重心和朝向与世界系对齐。物体6D位姿通常用原始物体所在世界系到相机系的RT变换来表示：

$$
T_{c}=R_{c m} * T_{m}+t_{c m}
$$

- $R_{cm}$代表由原始物体所在的世界系到相机系的旋转（或者说相机系相对于原始物体所在的世界系的旋转），$t_{cm}$代表由原始物体所在的世界系到相机系的平移，$T_m$表示原始物体所在的世界系下的3D点，$T_c$表示相机系下的3D点。

- <img src="../../../Library/Application Support/typora-user-images/截屏2021-09-28 下午1.48.51.png" alt="截屏2021-09-28 下午1.48.51" style="zoom:50%;" />

  

- 当世界系和物体本身对齐时，相机的6D位姿等价于物体的6D位姿。

  

  

  ![](https://img-blog.csdnimg.cn/20200513170910656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Rzb2Z0d2FyZQ==,size_16,color_FFFFFF,t_70#pic_center)

### 位姿与坐标系

- 物体的6D位姿是和物体3D模型本身的世界坐标系相关的，同一个相机系下，不同物体的6D位姿是不一样的；不同物体所在的世界系，一般对应不同的世界系源点，一般是各自物体的重心；因此同一个相机系下，不同物体的6D位姿是不一样的，不是指两个坐标系的变换，而是将目标物体从自身的坐标系变换到相机坐标系的RT变换。
- 6D位姿估计要求物体的3D模型已知的，求出的6D位姿也是将这个3D模型变换到相机坐标系下所需要的RT变换
- PnP计算的RT是相机坐标系到每个物体的世界坐标的变换，因为不同物体的世界坐标系可能不重合，因此会有多个RT
- 如果保持物体和相机的相对位置不变，则物体的位姿是不变的；如果物体不动，相机系发生了移动，则新的物体位姿需要在原来的基础上再叠加相机系的相对移动位姿 $\Delta R$和$\Delta t$

### 相机内参与图像坐标系

- 相机内参K矩阵是相机系下3D点坐标到图像坐标系uv的变换，主要包括焦距fx,fy和cx,cy。
- 相机外参指的是从世界坐标系到相机坐标系的变换,指代R,t
- 得到相机系的3D点坐标之后，通过相机内参矩阵和畸变矩阵可以将其投影到2D图片上

$$
\left(\begin{array}{l}
u \\
v \\
1
\end{array}\right)=\frac{1}{Z}\left(\begin{array}{ccc}
f_{x} & 0 & c_{x} \\
0 & f_{y} & c_{y} \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{l}
X \\
Y \\
Z
\end{array}\right) \triangleq \frac{1}{Z} \boldsymbol{K} \boldsymbol{P}\\
\\
P = TP_w = [R | t]P_w
$$

### PnP算法

- PnP(Perspective-n-Point)是求解 3D 到 2D 点对运动的方法。它描述了当我们知道 n 个 3D 空间点以及它们的投影位置时，如何估计相机所在的位姿。

- 参阅：《SLAM14讲》7.1.1

  ![截屏2021-09-27 下午4.12.04](../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午4.12.04.png)

## 经典论文

### 综述

- 大多数单目RGB 6D位姿估计的深度学习方法是基于两种思路
  - 首先检测2D目标，通过3D-2D点对的联系，使用PnP算法求解6D位姿（最小二乘法解超定方程）
    - 稀疏点对：关键点/3D框角点
    - 稠密点对：物体的所有像素，结果更加鲁棒
  - 直接回归6D位姿 [R | t]
    - 目前的热点和主流算法

### *Real-Time Seamless Single Shot 6D Object Pose Prediction*

- 主要思想：
  - 提出了一种使用一张2D图片来预测物体6D姿态的方法。但是，并不是直接预测这个6D姿态，而是通过先预测3D bounding box在2D图像上的投影的1个中心点和8个角点，然后再由这9个点通过PnP算法计算得到6D姿态。
  - 不关心怎么由PNP算法得到物体的6D姿态，而只关心怎么预测一个物体的3D bounding box在2D图像上的投影，即9个点的预测。于是把预测6D姿态问题转为了预测9个坐标点的问题。
  - 类似于yolo算法，也是将图片分成SxS的网格，每个网格负责预测物体中心落在此格的物体的9个投影点。网络输出为SXSXD。

- CVPR 2018 https://github.com/Microsoft/singleshotpose

### PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scene

- 会议：Robotics: Science and Systems (RSS) 2018

- 主要思想

  - 多分支预测任务，包括
    - 语义标签，即mask
    - 3D平移向量预测。先通过霍夫投票锁定中心点的位置。霍夫投票在本文中就是每个像素点预测一个向量，认为该物体中心点就在这个向量上，对处于该位置的点都投一票，每个像素都处理完毕之后就能得到一个得分最高的位置，认为该位置是物体中心点，从而确定(cx, cy)。然后预测每个像素点的深度，则物体中心点的深度$T_z$就是给它投票的这些像素点的平均深度，根据单孔相机成像方程可以求出平移向量$T = (T_x, T_y, T_z)$。注意，物体中心的相机下3D坐标就是物体中心所经过的平移向量。

  $$
  \left[\begin{array}{l}
  c_{x} \\
  c_{y}
  \end{array}\right]=\left[\begin{array}{l}
  f_{x} \frac{T_{x}}{T_{z}}+p_{x} \\
  f_{y} \frac{T_{y}}{T_{z}}+p_{y}
  \end{array}\right]
  $$
  <img src="../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午8.02.07.png" alt="截屏2021-09-27 下午8.02.07" style="zoom:50%;" />

  

  

  - 使用霍夫投票层获取的物体bbox经过roi pooling层和全连接层,回归出代表旋转的四元数。
  - 损失函数
    - Pose Loss用于验证回归出的R'是否正确，计算使用R'旋转出的模型的特定点与使用GT旋转出的模型的特征点的距离.缺点是不能处理对称。为此引入的SLOSS
    - ShapeMatch Loss

$$
\operatorname{SLoss}(\tilde{\mathbf{q}}, \mathbf{q})=\frac{1}{2 m} \sum_{\mathbf{x}_{1} \in \mathcal{M}} \min _{\mathbf{x}_{2} \in \mathcal{M}}\left\|R(\tilde{\mathbf{q}}) \mathbf{x}_{1}-R(\mathbf{q}) \mathbf{x}_{2}\right\|^{2}
$$

- 一个缺点：一个类只能预测一个物体

### *SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again*

- 主要思想：**从2D框建立6D假设**

  - 把6D位姿解耦成平面内旋转和视角
  - 网络输出2D box,每个检测框附带着最可能的一些6D位姿池
  - 为了表示位姿，通过解析网络输出的视角score和平面内rotation，并使用投影属性来初始化6D位姿的假设
  - 最后对每个框内物体在它的6D位姿池进行refine，并且选择最佳的一个（通过不断尝试和验证，最小化重投影误差）

  $$
  \underset{R, t}{\arg \min } \sum_{i}\left(\left\|\pi\left(R \cdot X_{i}+t\right)-y_{i}\right\|^{2}\right)
  $$

- ICCV 2017 https://github.com/wadimkehl/ssd-6d (tf)

![截屏2021-09-14 下午4.39.48](../../../Library/Application Support/typora-user-images/截屏2021-09-14 下午4.39.48.png)



### EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach

- 主要思想：以2D检测框架为基础，以EfficientNet为backbone，包含双边特征金字塔网络，以及多个预测的子网络。主要创新点在于多级特征融合。

  ![截屏2021-09-22 下午5.33.01](../../../Library/Application Support/typora-user-images/截屏2021-09-22 下午5.33.01.png)

- Rotation Network采用了初始化-迭代优化的策略，前几个卷积块输出一个初始化旋转向量r，通过refinement module的数次迭代优化输出最终的r

  ![截屏2021-09-22 下午5.39.20](../../../Library/Application Support/typora-user-images/截屏2021-09-22 下午5.39.20.png)

- Translation Network采用了类似于posecnn的做法，先预测中心点的2D坐标，以及tz分量，然后通过相机内参来计算平移向量t的x,y分量得到完整的t。

- 一点疑惑：多级特征融合每一级都输出了subnets，目标不是很明确

### HybridPose: 6D Object Pose Estimation under Hybrid Representations

![截屏2021-09-27 下午8.13.34](../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午8.13.34.png)

- 基本信息
  
  - **CVPR 2020**
  - **code：https://github.com/chensong1995/HybridPose**
  
- HybridPose利用混合中间表示在输入图像中表达不同的几何信息，包括关键点，边缘向量和对称对应关系。与单一表示相比，当一种类型的预测表示不准确时（例如，由于遮挡），混合表示允许姿势回归利用更多不同的特征。 与SOTA的姿态估计方法相比，HybridPose在运行时间上具有优势，并且准确性更高。

- 混合特征
  - 关键点。去预测预定义好的K个关键点的坐标；主力特征。HybridPose使用了称为PVNet的现成模型，该模型是基于关键点的最新姿态估计器，可以采用投票方案来预测可见和不可见的关键点。
  - 边缘。在预定义图上的边缘向量，对每一对关键点进行显式地建模，模拟每对关键点之间的位移，数量是C(K,2)；起到stabilize的作用 ；
  - 对称性对应。第三中间表示由反映基础反射对称性的预测像素方向对称性对应组成。对称点的对应关系，预测对应对称点对，数量很庞大，用mask内部的像素流表示。

- 位姿回归

  - HybridPose的第二个模块将预测的中间表示{K，E，S}作为输入，并为输入图像I输出6D对象姿态R,t
  - 初始化子模块：使用EPnP算法
  - 优化子模块：最小化投影误差

  

### GDR-Net

- 基本信息：

  - CVPR 2021 
  - 作者：清华大学 王谷等

  ![截屏2021-09-27 下午4.05.44](../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午4.05.44.png)

- 主要思想

  - 3D旋转的参数化：旋转参数小于5导致欧氏空间内不连续，而神经网络对于连续量比离散量更好学习，故本文采用6参数回归法，用预测出的6个参数，再根据SO(3)性质计算出另外三个参数。

  - 3D平移的参数化

    - 采用类似于PoseNet的方法，仍然预测物体中心点和中心点的深度，用这三个坐标来计算物体位姿的translation。

    - 由于采用了检测后的zoom-in RoI（使用zoom-in的原因是，对于R来说，减少了背景信息的影响。对于T，减少了其搜索空间），尺度适应性是必须要考虑的问题。

      

    - ![截屏2021-09-27 下午3.58.00](../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午3.58.00.png)

    

  - 中间特征的解耦合：3种几何信息作为中间监督信息

    - 将Zoom-in图像送到ResNet34编码器中，再将高维信息上采样回64x64尺寸的特征图，作为几何信息的中间表示。
    - Dense Correspondences Maps $M_{2D-3D}$: 为了计算稠密2D-3D对应图，首先估计稠密3D坐标图$M_{XYZ}$（归一化到[0,1]），然后把$M_{XYZ}$ stack到对应的2D像素坐标上。
    - Surface Region Attention Maps (MSRA)： 将每个像素点划分到对应区域，也就是把表面分片，从图上看，应该是划分成了65个区域。该特征是用来隐式地表示物体的对称信息。（For instance, if a pixel is assigned to two potential fragments due to a plane of symmetry, Minimizing this assignment will return a probability of 0.5 for each fragment. ）
    - $M_{vis}$: 物体的分割图，作为辅助的冗余信息。

  - 损失函数
    - 总体目标是位姿损失和几何特征损失
      - 位姿损失：分为旋转损失（考虑了对称）、物体中心2D坐标损失以及中心点深度损失
      - 几何信息损失：包括物体范围内的3D对应点的损失、物体分割损失、3D点所属区块的交叉熵损失（因为类似于分类故而采用交叉熵）

  ![截屏2021-09-27 下午3.59.14](../../../Library/Application Support/typora-user-images/截屏2021-09-27 下午3.59.14.png)

### 关于loss

- 由果推因的过程：因和果距离比较远的时候很难去学习=》中间特征（转化到某一个空间）；

- 解耦合：各个分量的加权，这些分量尽可能正交；特征在不同层次的空间呈现（）

  

  交叉点 AR/AI  领域探讨

  摘要翻译

